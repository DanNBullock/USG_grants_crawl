{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16a995a5",
   "metadata": {},
   "source": [
    "# Generic Data Source\n",
    "## Processing and Figure Production Template\n",
    "\n",
    "A template workflow for the analysis of federal funding information (namely grant records) for open-science-related endeavors.\n",
    "\n",
    "### Overview\n",
    "\n",
    "This notebook is designed to make use of the [code toolset]() provided with this repository and, in doing so, apply a customized data acquisition and processing workflow on your local system (which is presumed to not have the requisite data files--at least not the first time you run this notebook).  Once this data has been acquired and processed, some standard analyses are conducted and associated figures are generated.\n",
    "\n",
    "#### A warning\n",
    "\n",
    "Each of the workflow steps (e.g. acquisition, preprocessing, analysis, figure generation) takes a nontrivial amount of local storage space (e.g. 150 GB in the case of NIH data) and a nontrivial amount of processing time.  Be aware of this as you seek to enact this notebook.\n",
    "\n",
    "### File paths and assumptions\n",
    "\n",
    "We'll start by establishing a number of parameters and filepaths that will be used throughout this process.  **Be sure to inspect these to ensure that they are consistent with your local setup before proceeding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f580dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# we're going to assume that this notebook is being run from the /notebooks directory of the repository\n",
    "# if this is not the case then you will need to adjust the path to the /inputData directory, which should be in the same directory as the /notebooks directory\n",
    "inputDataDir='..' + os.sep + 'inputData' + os.sep\n",
    "\n",
    "# relatedly, we will also need to make extensive use of code that has been developed for this project.\n",
    "# this code is stored within the /src directory of the repository\n",
    "codeDir='..' + os.sep + 'src' + os.sep\n",
    "# we will need to add this code directory to the python path so that we can import the code\n",
    "import sys\n",
    "sys.path.append(codeDir)\n",
    "import analyzeData\n",
    "import getData\n",
    "import processData\n",
    "import figs\n",
    "\n",
    "# within the inputDataDir, there could conceivably be a data directory for each of the potential data sources that this toolset was designed to work with\n",
    "# these include: grants.gov, NIH, NSF, and USAspending\n",
    "dataSources=['grantsGov','NIH','NSF','USAspending']\n",
    "# we'll go ahead and select one of these using an integer index.  We'll default to the first one in the list, which is grants.gov\n",
    "defaultDataIndex=0\n",
    "\n",
    "# inside each of these data source directories, there should be a set of sub-directories corresponding to various phases of the workflow.  These are:\n",
    "dataFolders=['raw','processed','analyzed','figures']\n",
    "# the raw directory should contain the raw data files as downloaded from the data source\n",
    "# the processed directory should contain the data files after they have been processed by the appropriate data processing functions for that data set\n",
    "# the analyzed directory should contain the data derivitives of standard analysis functions\n",
    "# the figures directory should contain the figures depicting the results of the various analyses\n",
    "\n",
    "# additionally, one consequence of selecting a data source is that we need to go ahead and specify dataset-specfific fields or values that will come in to play lter\n",
    "# data dictionary locations\n",
    "# grants.gov : https://www.grants.gov/help/html/help/XMLExtract/XMLExtract.htm\n",
    "# NIH : https://report.nih.gov/exporter-data-dictionary\n",
    "# NSF : https://www.nsf.gov/awardsearch/resources/Award.xsd\n",
    "# USAspending : https://www.usaspending.gov/data-dictionary\n",
    "\n",
    "# Each data set, when split in to individual xml records (except for usaspending, which we don't do that for) needs to have a root element.\n",
    "# Because most of the data sets didn't come as xml files, we arbitrarily set a root element that's the same for all of these, us ing grants.gov as the standard, which is 'rootTag'\n",
    "rootTags=['rootTag','rootTag','rootTag','NotApplicable']\n",
    "\n",
    "# for example, when looking for sub-organizations, each data-set has a different \n",
    "# given that the relevant field could be deeper or shallower for each set, we'll specify it as a list of lists, indicating the sequence of fields and subfields that need to be traversed to get to the relevant field\n",
    "subOrgFields=[[['AgencyCode']],[['IC_NAME']],[['Award'],['Organization'],['Directorate'],['LongName']],[['TBD']]]\n",
    "\n",
    "# similarly there's the field for the \"name\" or identifier for the record\n",
    "nameFields=[[['OpportunityID']],[['ACTIVITY']],[['Award'],['AwardID']],[['award_id_fain']]]\n",
    "\n",
    "# also specify the field for the description\n",
    "descriptionFields=[[['Description']],[['ABSTRACT_TEXT']],[['Award'],['AbstractNarration']],[['description']]]\n",
    "\n",
    "# for now though we must select a single data source.  For the purposes of this template, we wil default to the first exemplar in the dataSources list, which is grants.gov\n",
    "defaultDataSource=dataSources[defaultDataIndex]\n",
    "\n",
    "# additionally, one of the main goals / analyses of this project is to identify which funding records correspond to open-science endeavors\n",
    "# The first-pass approach for this is to use a set of keywords as search targets within descriptions of the funding records.\n",
    "# However several such keyword lists have been developed for this project.  These include:\n",
    "keyWordOptions=['keywords.csv','OSterms_LeeChung2022.csv','GPT_OS-Terms.csv','policy-derived_keywords.csv']\n",
    "# as you may have surmised, each of these is a csv file containing a list of keywords.  The last column are the keywords themselves, while any preceeding colums correspond to higher-order categories that the keywords may be grouped into\n",
    "# for now though we must select a single keyword list.  For the purposes of this template, we wil default to the second exemplar in the keyWordOptions list, which is OSterms_LeeChung2022.csv\n",
    "defaultKeyWordList=keyWordOptions[3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc3bcc84",
   "metadata": {},
   "source": [
    "### Data acquisition\n",
    "\n",
    "Each of the established data sources has an online repository which can be used to download the entirety of the corresponding data.  However, these are not standardized, and so it is necessary to use a custom created set of functions to download the corresponding data.  That is what we will do in the next section.\n",
    "\n",
    "In order to ensure that we aren't wasting effort though, we'll be sure to check if there is already data in the 'raw' directory.  If so, we'll skip the downloading and proceed to the pre-processing phase.\n",
    "\n",
    "**NOTE**: If it is necessary to download the data, this may take a particularly long time.  Additionally, depending on the data provider, it may be necessary to re-initiate the download process more than once to ensure a complete download of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1308420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "# first we set the path to the raw data directory for the selected data source\n",
    "currentRawDataDir=os.path.join(inputDataDir,defaultDataSource,dataFolders[0])\n",
    "# let's see if the directory exists and if so, if there are at least one compressed file (e.g. .zip, .gz, .tar, etc.) and one xml file (which would presumably be the uncompressed, and unprocessed data)\n",
    "# if not, then we will need to download the data from the data source\n",
    "rawDirContainsZIP=glob(currentRawDataDir + os.sep + '*.zip')\n",
    "rawDirContainsGZ=glob(currentRawDataDir + os.sep + '*.gz')\n",
    "rawDirContainsTAR=glob(currentRawDataDir + os.sep + '*.tar')\n",
    "rawDirContainsCompressed=rawDirContainsZIP + rawDirContainsGZ + rawDirContainsTAR\n",
    "rawDirContainsXML=glob(currentRawDataDir + os.sep + '*.xml')\n",
    "# if we fail to find at least one compressed file or one xml file, then we will need to download the data from the data source\n",
    "if (len(rawDirContainsCompressed)==0) or (len(rawDirContainsXML)==0):\n",
    "    print('Search of raw data directory\\n' + currentRawDataDir + '\\n failed to find at least one compressed file and one xml file.  Downloading data from data source.')\n",
    "    getData.getDataFromRemoteSource(currentRawDataDir,defaultDataSource)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92b3271e",
   "metadata": {},
   "source": [
    "## Data processing and cleaning\n",
    "Assuming the data has been downloaded successfully, it now needs to be processed and, in some cases, cleaned and repaired.\n",
    "\n",
    "As before we can check to see if this has already been done.  If not, this may take a while.\n",
    "\n",
    "NOTE:  We'll also take this opportunity to check if dask is available on your system.  If it is we'll be able to parallelize some of our operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb258d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dask is available\n",
    "try:\n",
    "    import dask\n",
    "    daskAvailable=True\n",
    "except:\n",
    "    daskAvailable=False\n",
    "\n",
    "\n",
    "\n",
    "# the resultant data should be stored in the grantsGov/processed directory\n",
    "processedDataPath=os.path.join(inputDataDir,defaultDataSource,dataFolders[1])\n",
    "# now process it with processDownloadedData\n",
    "#  processDownloadedData(dataLocation,sourceOrg,saveDir,singleMulti='multi'):\n",
    "# implement a quick little timer to see how long this takes\n",
    "import time\n",
    "start=time.time()\n",
    "# do a quick check to see if the processed data directory already exists, and if there are any xml files in it\n",
    "processedDataDirExists=os.path.isdir(processedDataPath)\n",
    "# if it doesn't exist make it\n",
    "if not processedDataDirExists:\n",
    "    os.mkdir(processedDataPath)\n",
    "# now check to see if there are any xml files in it\n",
    "processedDataDirContainsXML=glob(processedDataPath + os.sep + '*.xml')\n",
    "# if there are no xml files in the directory, then we need to process the data\n",
    "if len(processedDataDirContainsXML)==0:\n",
    "    print('No xml files found in processed data directory.  Processing data from raw data.')\n",
    "    # if dask is available, then we can use it to speed up the processing\n",
    "    if daskAvailable:\n",
    "        # set up a dask client, and use all but 2 of the available cores\n",
    "        from dask.distributed import Client\n",
    "        import dask\n",
    "        # also initiate the progress bar\n",
    "        from dask.diagnostics import ProgressBar\n",
    "        pbar=ProgressBar()\n",
    "        pbar.register()\n",
    "        # check what the correct number of workers and threads should be given system resources\n",
    "        # first get the number of cores available\n",
    "        import multiprocessing\n",
    "        nCores=multiprocessing.cpu_count()\n",
    "        # now set the number of workers to be all but 2 of the available cores\n",
    "        nWorkers=nCores-2\n",
    "        # now set the number of threads per worker to be 1\n",
    "        threadsPerWorker=1\n",
    "        # now set up the client\n",
    "        client = Client(n_workers=nWorkers, threads_per_worker=threadsPerWorker)\n",
    "        # now process the data by running the processDownloadedData function with dask.delayed\n",
    "        #  processDownloadedData(dataLocation,sourceOrg,saveDir,singleMulti='multi'):\n",
    "        delayedProcessData=dask.delayed(processData.processDownloadedData)(currentRawDataDir,defaultDataSource,processedDataPath,singleMulti='multi')\n",
    "        # now run the delayed process\n",
    "        delayedProcessData.compute()\n",
    "        # now close the client\n",
    "        client.close()\n",
    "        # now close the progress bar\n",
    "        pbar.unregister()\n",
    "        \n",
    "    # if dask is not available, then we will have to process the data serially\n",
    "    else:\n",
    "        processData.processDownloadedData(currentRawDataDir,defaultDataSource,processedDataPath,singleMulti='multi')\n",
    "else:\n",
    "    print('xml files found in processed data directory.  Skipping processing.')\n",
    "# now let's see how long that took\n",
    "end=time.time()\n",
    "print('Processing took ' + str(end-start) + ' seconds.')\n",
    "\n",
    "\n",
    "import pickle\n",
    "# now also produce a single pickle file with all of the data in it using allXML_to_pickle(directoryPath,outFilePath)\n",
    "# set the path to where we want to save the pickle file\n",
    "outPickleFilePath=os.path.join(processedDataPath,'allData.pickle')\n",
    "# if the file already exists, then we don't need to do anything\n",
    "if not os.path.isfile(outPickleFilePath):\n",
    "    processData.allXML_to_pickle(processedDataPath,outPickleFilePath)\n",
    "else:\n",
    "    # if it does exist, then load it.\n",
    "    print('Pickle file already exists.  Loading.')\n",
    "    with open(outPickleFilePath,'rb') as f:\n",
    "        allData=pickle.load(f)\n",
    "        # now close the file\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6131c191",
   "metadata": {},
   "source": [
    "## Data quality assesment\n",
    "\n",
    "Now that we have downloded and (ostensibly) cleaned the data, it would be nice to get a sense of the quality of the data.  In this case, this means determining, for each standard record field, what proportion are \"empty\".  This will give us a sense of how \"complete\" the data is.  This is a good initial check of the data quality, though it does not give us a sense of the accuracy or reasonability of the data.  Checks of this sort for _soundness_ require a detailed understanding of the data, the context from which it arises, and the entities to which the records correspond.  Ideally, checks of soundness have been implemented in the preprocessing stage and have been used to implement corrections.\n",
    "\n",
    "Thus for this analysis, we'll print a table with the field names and number of empty entries.  To provide some context, the final row of the table will have the total number of record entries covered by the data set, which represents the maximal \"emptyness\" that could be acheived (i.e. if every record failed to have an entry for that field).\n",
    "\n",
    "We'll also plot this as a bar chart as well--our first figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# implement path to the \"analyzed\" and \"figures\" directories\n",
    "analyzedDataPath=os.path.join(inputDataDir,defaultDataSource,dataFolders[2])\n",
    "figuresDataPath=os.path.join(inputDataDir,defaultDataSource,dataFolders[3])\n",
    "\n",
    "# set the file paths to the files to be generated\n",
    "dataCompletenessDFPath=os.path.join(analyzedDataPath,'dataCompletenessDF.csv')\n",
    "dataCompletenessFigPath=os.path.join(figuresDataPath,'dataCompletenessFig.svg')\n",
    "# now check to see if the files already exist\n",
    "if 'allData' in globals():\n",
    "    # if they are dictionaries generated by the above pickle process, then the top level key is the file name,\n",
    "    # and we should convert this to a list of the underlying dictionary contents\n",
    "    # we'll do this using list comprehension\n",
    "    #xmlFileContentsOrPaths=[allData[iKeys] for iKeys in allData.keys()]\n",
    "    xmlFileContentsOrPaths=allData\n",
    "elif not 'allData' in globals() and os.path.isfile(outPickleFilePath):\n",
    "    # load the pickle file using pickle.load\n",
    "    allData=pickle.load(open(outPickleFilePath,'rb'))\n",
    "    # if they are dictionaries generated by the above pickle process, then the top level key is the file name,\n",
    "    # and we should convert this to a list of the underlying dictionary contents\n",
    "    # we'll do this using list comprehension\n",
    "    # xmlFileContentsOrPaths=[allData[iKeys] for iKeys in allData.keys()]\n",
    "    xmlFileContentsOrPaths=allData\n",
    "else:\n",
    "    # lets gather up all of the xml files in the processed data directory into a list\n",
    "    processedDataDirContainsXML=glob(processedDataPath + os.sep + '*.xml') \n",
    "    xmlFileContentsOrPaths=processedDataDirContainsXML\n",
    "\n",
    "if not os.path.isfile(dataCompletenessDFPath) or not os.path.isfile(dataCompletenessFigPath):\n",
    "    # if they don't exist, then we need to generate them\n",
    "    # lets gather up all of the xml files in the processed data directory into a list\n",
    "    # if the allData pickle file is in memory, then we can just pass that to quantifyDataCompleteness\n",
    "    # otherwise check if the pickle file exists on disk, and if so load it\n",
    "    # finally, if the pickle file doesn't exist for whatever reason, just pass the list of xml files\n",
    "\n",
    "    # now we pass this to quantifyDataCompleteness(inputData,fieldSequenceToSearch=None,maxDepth=3)\n",
    "    dataCompletenessDF=analyzeData.quantifyDataCompleteness(xmlFileContentsOrPaths,fieldSequenceToSearch=[rootTags[defaultDataIndex]],maxDepth=3)\n",
    "    # according to the documentation dataCompletenessDF should be: \n",
    "    #       A pandas dataframe with two columns:  the first column is the field name and the second column is the number of empty / null values for that field.\n",
    "    #       The last row is the total number of records assessed.\n",
    "    # for output, we'll present the table along side a bar plot of the number of empty / null values for each field\n",
    "    # use figs.plotNullValue_barPlot(dataCompletenessDF,figSize=(10,5)) for this\n",
    "    dataCompletenessFig=figs.plotNullValue_barPlot(dataCompletenessDF,figSize=None,logScale=None,)\n",
    "    # set up the display for the side by side table and figure for jupyter notebooks\n",
    "    # save both the dataCompletenessDF and the dataCompletenessFig\n",
    "    dataCompletenessDF.to_csv(dataCompletenessDFPath)\n",
    "    dataCompletenessFig.savefig(dataCompletenessFigPath)\n",
    "else:\n",
    "    # if they do exist, then we need to load them\n",
    "    dataCompletenessDF=pd.read_csv(dataCompletenessDFPath,index_col=0)\n",
    "    # still get the file paths\n",
    "    processedDataDirContainsXML=glob(processedDataPath + os.sep + '*.xml') \n",
    "\n",
    "# in either case, we will want to display the table and figure\n",
    "from IPython.display import display, SVG\n",
    "\n",
    "\n",
    "\n",
    "# display 30 rows of the data completeness table\n",
    "# display(dataCompletenessDF.head(30))\n",
    "# display(SVG(dataCompletenessFigPath))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c1f7d27",
   "metadata": {},
   "source": [
    "### How do we find \"Open Science Infrastructure\"-related grants?\n",
    "\n",
    "How can we tell which of these entries are related to-Open Science Infrastructure?  In eachd data set there is a field which provides information about the nature or purpose of the funded endeavor, typically as a sizeable block of text (e.g. `Description` in grants.gov or `abstract` in the NIH data set).  Ultimately, this field can provide a source of information that can be used in filtering or categorizing the entries.  If we can find a way to use the information contained within the description to determine whether or not the grant is related to this topic, we might be able to limit our consideration to this subset, and thereby be able to make insights about its characteristics.\n",
    "\n",
    "Before we dive too deeply into using the `Description` it may be best for us to perform a sanity check and see how much information is contained within this field across the various grant entries. For this approach to be viable, a sufficient number of the grant descriptions will need to have a reasonable amount of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc07b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "nameFieldSequence=[]\n",
    "nameFieldSequence.extend([rootTags[defaultDataIndex]])\n",
    "[nameFieldSequence.extend(x) for x in nameFields[defaultDataIndex]]\n",
    "currNameFieldFull=nameFieldSequence\n",
    "\n",
    "awardDescriptionFieldSequence=[]\n",
    "awardDescriptionFieldSequence.extend([rootTags[defaultDataIndex]])\n",
    "[awardDescriptionFieldSequence.extend(x) for x in descriptionFields[defaultDataIndex]]\n",
    "currAwardDescriptionFieldFull=awardDescriptionFieldSequence\n",
    "\n",
    "# check to see if the wordCountDF and wordCountFig already exist\n",
    "wordCountDFPath=os.path.join(analyzedDataPath,'wordCountDF.csv')\n",
    "wordCountFigPath=os.path.join(figuresDataPath,'wordCountFig.svg')\n",
    "if not os.path.isfile(wordCountDFPath) or not os.path.isfile(wordCountFigPath):\n",
    "    wordCountDF=analyzeData.wordCountForField([xmlFileContentsOrPaths[iKey]for iKey in list(xmlFileContentsOrPaths.keys())],targetField=currAwardDescriptionFieldFull,nameField=currNameFieldFull,savePath=None)\n",
    "    # according to the documentation wordCountDF should be:\n",
    "    # A pandas dataframe with two columns: 'itemID' and 'wordCount'. The 'itemID' column contains the name of the input structure, and the 'wordCount'\n",
    "    #        column contains the word count of the target field for each input structure.\n",
    "    # now we'll plot the wordCountDF as a histogram\n",
    "    wordCountFig=figs.plotWordCount_histogram(wordCountDF,figSize=(10,5))\n",
    "    # save both the wordCountDF and the wordCountFig\n",
    "    wordCountDF.to_csv(wordCountDFPath)\n",
    "    wordCountFig.savefig(wordCountFigPath)\n",
    "\n",
    "# in either case, load and display the wordCountDF and wordCountFig\n",
    "wordCountDF=pd.read_csv(wordCountDFPath,index_col=0)\n",
    "# display 30 rows of the wordCountDF\n",
    "# display(wordCountDF.head(30))\n",
    "# display(SVG(wordCountFigPath))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "121dcd99",
   "metadata": {},
   "source": [
    "### Searching for Open Science terms\n",
    "\n",
    "Now that we have a sense of the information sources (i.e. descriptions) that we might be able to use to categorize or classify the entries, we can move towards actually applying our classification / categorization method.\n",
    "\n",
    "Our goal is to find those entries that are specific to Open Science (and open science infrastructure).  How would we go about doing this?\n",
    "\n",
    "We can make a first, naieve attempt at this by using a word-bank with words or phrases that we believe are likely to show up in a grant relevant to open science infrastructure (e.g. 'metadata', 'FAIR', 'data sharing', etc.). In this repository we have just such a word-bank already available (in keywords.txt), but users should feel free to update and/or augment it as they see fit.\n",
    "\n",
    "To begin then, we'll do a quick search to see how many grants each of the word-bank terms comes up in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87344d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#analyzedDataPath=os.path.join(inputDataDir,defaultDataSource,dataFolders[2])\n",
    "#figuresDataPath=os.path.join(inputDataDir,defaultDataSource,dataFolders[3])\n",
    "# create the path to the save product of the regex search\n",
    "regexSearchSavePath=os.path.join(analyzedDataPath,'regexSearchResults.h5')\n",
    "# create the ptah to the csv containing the search terms, which should be in the main directory as of 07/12/2023\n",
    "stringPhraseListPath=os.path.join('../keywordLists/',defaultKeyWordList)\n",
    "# load the stringPhraseList\n",
    "stringPhraseList=pd.read_csv(stringPhraseListPath)\n",
    "# get the last column of the stringPhraseList and use it as the keywordList\n",
    "keywordList=list(stringPhraseList.iloc[:,-1].values)\n",
    "# unfortunately, we're having to implement a little check here to see if any of the keyword are a single character, which will return way too many results.  In this way, we only want to search for multi character keywords.\n",
    "# so we'll loop through the keywordList and check if any of the keywords are a single character\n",
    "# if they are, then we'll remove them from the keywordList\n",
    "[keywordList.remove(keyword) for keyword in keywordList if len(keyword)==1]\n",
    "\n",
    "\n",
    "# check if the regexSearchSavePath file exists, and if so, don't rerun the regex search\n",
    "if os.path.exists(regexSearchSavePath):\n",
    "    print('regexSearchSavePath file already exists.  Skipping regex search.')\n",
    "else:\n",
    "    # run the regex search\n",
    "    # regexSearchAndSave(processedDataPath,keywordList,targetField,nameField='infer',rootTag='root\n",
    "    analyzeData.regexSearchAndSave(xmlFileContentsOrPaths,keywordList,currAwardDescriptionFieldFull,daskify=False,savePath=regexSearchSavePath)\n",
    "# load up the results\n",
    "# try it both ways, one way should work\n",
    "try:\n",
    "    print('Attempting h5py load')\n",
    "    import h5py\n",
    "    regexSearchResults=h5py.File(regexSearchSavePath,'r')\n",
    "    # remember:\n",
    "    # efficientDict['rowDescription']='Searched Keywords'\n",
    "    # efficientDict['colDescription']='Award Number'\n",
    "    # dataKeys=['dataMatrix','rowName','colName']\n",
    "    # attributeKeys=['rowDescription','colDescription']\n",
    "    # so we'll get the dataMatrix from the regexSearchResults as a numpy array\n",
    "    regexSearchResultsDataMatrix=regexSearchResults['dataMatrix'][:]\n",
    "    # and the rowName, but remember they come in as bytes, so we'll need to decode them\n",
    "    regexSearchResultsRowNames=[rowName.decode('utf-8') for rowName in regexSearchResults['rowName'][:]]\n",
    "    # we don't need the colName as these are the award numbers and we don't need these\n",
    "    # create a save path for the keywordCounts DF\n",
    "    keywordCountsSavePath=os.path.join(analyzedDataPath,'keywordCounts.csv')\n",
    "    # create the keywordCountsDF\n",
    "    # because each row is a keyword, you can just sum across the columns to get the counts for that keyword\n",
    "    # the keywords themselves come from the regexSearchResultsRowNames\n",
    "    # make sure the DF has column names of 'itemID' and 'count'\n",
    "    keywordRegexBoolMatrixDF= pd.DataFrame(data={'itemID':regexSearchResultsRowNames,'count':regexSearchResultsDataMatrix.sum(axis=1)})\n",
    "except:\n",
    "    print('Attempting custom pandas load')    \n",
    "    import h5py\n",
    "    regexSearchResults=h5py.File(regexSearchSavePath,'r')\n",
    "    # remember:\n",
    "    # dataKeys=['dataMatrix','rowName','colName']\n",
    "    # attributeKeys=['rowDescription','colDescription']\n",
    "    # so we'll get the dataMatrix object from the regexSearchResults\n",
    "    regexSearchResultsDataMatrix=regexSearchResults['dataMatrix']\n",
    "    # now use analyzeData.pdDataFrameFromHF5obj(hf5obj)\n",
    "    # to get the keywordCountsDF\n",
    "    keywordRegexBoolMatrixDF=analyzeData.pdDataFrameFromHF5obj(regexSearchResultsDataMatrix)\n",
    "\n",
    "# now that we have the keywordRegexBoolMatrixDF, in which the column names are the award numbers (as strings) and the row names are the keywords, we can sum across the columns to get the counts for each keyword\n",
    "# we'll do this by using the .sum() method on the keywordRegexBoolMatrixDF\n",
    "# we want the resultant keywordCountsDF to have columns 'itemID' and 'count'\n",
    "keywordCountsDF=pd.DataFrame(data={'itemID':keywordRegexBoolMatrixDF.index,'count':keywordRegexBoolMatrixDF.sum(axis=1)})\n",
    "# looks like we need to reset the index\n",
    "keywordCountsDF=keywordCountsDF.reset_index(drop=True)\n",
    "\n",
    "# now we'll plot the keywordCountsDF as a bar plot in seaborn\n",
    "# keywordCount_barPlot(keywordCountDF,figSize=(10,5),fig=None,ax=None,figSavePath=None)\n",
    "# create the path to save the figure\n",
    "keywordCountsFigSavePath=os.path.join(figuresDataPath,'keywordCountsFig.svg')\n",
    "keywordCountsFig=figs.keywordCount_barPlot(keywordCountsDF,figSize=None,logScale=None,fig=None,ax=None,figSavePath=keywordCountsFigSavePath)\n",
    "# display the keywordCountsDF and the keywordCountsFig\n",
    "# display 30 rows of the keywordCountsDF\n",
    "# display(keywordCountsDF.iloc[:30,:])\n",
    "#display(keywordCountsFig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a9a2d15",
   "metadata": {},
   "source": [
    "### What counts and what doesn't\n",
    "\n",
    "Now that we have a broad overview of the frequency at which the relevant terms are occuring in the data set, perhaps we should take a step back and consider what we are visualizing in the above figure.\n",
    "\n",
    "Namely, for each of these columns, the number corresponds to the number of records which have the relevant term.  Does this mean that the record is related to open science or open science infrastructure?  Not at all.  It could be that the entry in question uses the term in passing, or in a context that is distinct from open science / infrastructure. Presumably though, a \"legitimately\" open science entry will likely mention _several_ open science related terms (assuming our keyword list is comprehensive / representative enough).  To that end, we may wish to get a sense of what the distribution of keyword hits is across our entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c719d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we'll use the plotting function matrix_histogramCounts(inputMatrix,keepAxis='columns',dropZero=True,binSize=1,figSize=(10,5),fig=None,ax=None,figSavePath=None) for this\n",
    "# create the path to save the figure\n",
    "keywordCountsMatrixFigSavePath=os.path.join(figuresDataPath,'keywordCountsMatrixFig.svg')\n",
    "keywordCountsMatrixFig,figAxis,histDF=figs.matrix_histogramCounts(keywordRegexBoolMatrixDF,keepAxis='columns',dropZero=True,binSize=1,figSize=(10,5),fig=None,ax=None,figSavePath=keywordCountsMatrixFigSavePath)\n",
    "# display the histDF as well\n",
    "# but only 30 rows\n",
    "#histDF.head(30)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "595e1427",
   "metadata": {},
   "source": [
    "### A closer look at the select few\n",
    "\n",
    "Given the distribution of values in the previous plot, it seems like 3 might serve as a reasonable threshold, such that we are only looking at records with three or more key terms in them.  \n",
    "\n",
    "#### ... And a further sanity check\n",
    "\n",
    "As a further sanity check, we can also present an interactive table (assuming this is being viewed / interacted with as a jupyter notebook and not a static report)  We'll plot these in *reverse* order such that the records with the _highest_ number of hits are presented first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e729d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xmltodict\n",
    "import h5py\n",
    "# we'll begin with the creation of the thresheld dataset\n",
    "thresholdValue=3\n",
    "# now create a path to the hdf5 file that will contain the thresholded dataset\n",
    "thresholdedDataSavePath=os.path.join(analyzedDataPath,'thresholdedData.hd5')\n",
    "# check if the thresholdedDataSavePath file exists. If it doesn't exist, then we'll apply the threshold\n",
    "# if it does exist, then we'll load it up\n",
    "if not os.path.exists(thresholdedDataSavePath): \n",
    "\n",
    "    # get the indexes of the columns that have at least thresholdValue or more keyword hits when keywordRegexBoolMatrixDF (a dataframe) is summed across the rows\n",
    "    thresholdSurvivorIndexes=np.where(keywordRegexBoolMatrixDF.sum(axis=0)>=thresholdValue)[0]\n",
    "    # now we'll use these indexes to remove those columns from keywordRegexBoolMatrixDF that have less than thresholdValue keyword hits, leaving only the columns that have at least thresholdValue keyword hits\n",
    "    thresholdedData=keywordRegexBoolMatrixDF.iloc[:,thresholdSurvivorIndexes]\n",
    "    # save the dataframe to hdf5\n",
    "    thresholdedData.to_hdf(thresholdedDataSavePath,'thresholdedData')\n",
    "# in either case, we will want to load the thresholdedData, using pdDataFrameFromHF5obj\n",
    "# load the object using h5py\n",
    "loadedHDF5obj=h5py.File(thresholdedDataSavePath,'r')\n",
    "thresholdedData=analyzeData.pdDataFrameFromHF5obj(loadedHDF5obj[list(loadedHDF5obj.keys())[0]])\n",
    "\n",
    "# create the path to the postThresholdInspectionDF\n",
    "postThresholdInspectionDFPath=os.path.join(analyzedDataPath,'postThresholdInspectionDF.csv')\n",
    "# if it doesn't exist, make it\n",
    "if not os.path.exists(postThresholdInspectionDFPath):\n",
    "    # create a dataframe that will hold the post threshold records and some information about them\n",
    "    # the columns should be 'awardNumber','keywordsFound','subOrg','description'\n",
    "    postThresholdInspectionDF=pd.DataFrame(columns=['awardNumber','keywordNum','keywordsFound','subOrg','description'])\n",
    "    # now we'll iterate across the columns of thresholdedData, to obtain information about the awards that have at least thresholdValue keyword hits\n",
    "    for iColumns in thresholdedData.items():\n",
    "        # get the awardNumber\n",
    "        awardNumber=iColumns[0]\n",
    "        # find the indexes of the rows (terms) that are non-zero\n",
    "        nonZeroRowIndexes=np.where(iColumns[1].values==True)[0]\n",
    "        # use this to create a list of the keywords that were hit, the keywords are the row names\n",
    "        keywordsHit=list(thresholdedData.index[nonZeroRowIndexes])\n",
    "            # also get the number of keywords that were hit\n",
    "        keywordNum=len(keywordsHit)\n",
    "        \n",
    "        # look in the pickle object using the awardNumber as the key to find the xml dictionary, which is stored in xmlFileContentsOrPaths\n",
    "        currXMLdictionary=xmlFileContentsOrPaths[awardNumber]\n",
    "\n",
    "        # NOTE: not elegant, but it works\n",
    "        awardDescriptionFieldSequence=[]\n",
    "        awardDescriptionFieldSequence.extend([rootTags[defaultDataIndex]])\n",
    "        [awardDescriptionFieldSequence.extend(x) for x in descriptionFields[defaultDataIndex]]\n",
    "        #awardDescriptionFieldSequence.extend(descriptionFields[defaultDataIndex])\n",
    "        awardSubOrgFieldSequence=[]\n",
    "        awardSubOrgFieldSequence.extend([rootTags[defaultDataIndex]])\n",
    "        [awardSubOrgFieldSequence.extend(x) for x in subOrgFields[defaultDataIndex]]\n",
    "        #awardSubOrgFieldSequence.extend(subOrgFields[defaultDataIndex])\n",
    "        # print both of them for debugging\n",
    "        # now extract the description and sub-organization\n",
    "        awardDescription=analyzeData.extractValueFromDictField(currXMLdictionary,awardDescriptionFieldSequence)\n",
    "        awardSubOrg=analyzeData.extractValueFromDictField(currXMLdictionary,awardSubOrgFieldSequence)\n",
    "        # add everything to the existing dataframe using concat\n",
    "        postThresholdInspectionDF=pd.concat([postThresholdInspectionDF,pd.DataFrame([[awardNumber,keywordNum,keywordsHit,awardSubOrg,awardDescription]],columns=['awardNumber','keywordNum','keywordsFound','subOrg','description'])],ignore_index=True)\n",
    "\n",
    "    # save it down\n",
    "    postThresholdInspectionDF.to_csv(os.path.join(analyzedDataPath,'postThresholdInspectionDF.csv'))\n",
    "\n",
    "# either way load it up\n",
    "postThresholdInspectionDF=pd.read_csv(os.path.join(analyzedDataPath,'postThresholdInspectionDF.csv'),index_col=0)\n",
    "# now display postThresholdInspectionDF in an interactive table\n",
    "from IPython.display import display\n",
    "# display 30 rows of the dataframe\n",
    "# display(postThresholdInspectionDF.head(30))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4d530fb",
   "metadata": {},
   "source": [
    "### Considering higher order patterns\n",
    "\n",
    "Now that we have looked at the records in an individual fashion, we can begin trying to enrich this consideration by looking at the _context_ in which these terms are being found.  for example, some questions we can ask are:\n",
    "\n",
    "- 1. What other terms is this term found with?\n",
    "- 2. What sub-organizations are associated with uses of the term.\n",
    "\n",
    "In the next section we'll make an effort to quantify and visualize these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc5e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now that we have filtered the data to only be considering relevant entries, we can look at the contexts in which the terms are co-occuring, either with respect to other terms or with respect to organizational sub-divisions.\n",
    "\n",
    "in order to assess the term-term co-occurance we will use:\n",
    "coOccurrenceMatrix(occurenceMatrix,rowsOrColumns='rows',savePath='')\n",
    "however, assessing the term-agency co-occurance will be a bit more complicated, and we will need to extract the grantID-agency data using \n",
    "fieldExtractAndSave(inputStructs,targetField,nameField='infer',savePath='')\n",
    "\n",
    "The fist step for either of these is to create a path to the saved product, and determine if it already exists\n",
    "\"\"\"\n",
    "# create the path to the save product of the coOccurrenceMatrix\n",
    "# note that we're saving it as a csv, because it should be pretty small given that we have a relatively small number of terms.\n",
    "coOccurrenceMatrixSavePath=os.path.join(analyzedDataPath,'coOccurrenceMatrix.csv')\n",
    "# create the path to the save product of the fieldExtractAndSave, remember that what we will be searching for in this case is agency\n",
    "agencyExtractSavePath=os.path.join(analyzedDataPath,'agencyExtract.csv')\n",
    "# finally, create a save path for the agency-term co-occurrence matrix\n",
    "agencyTermCoOccurrenceMatrixSavePath=os.path.join(analyzedDataPath,'agencyTermCoOccurrenceMatrix.csv')\n",
    "\n",
    "# we'll start with the easy case: co-occurrence of terms\n",
    "# check if the coOccurrenceMatrixSavePath file exists. If it doesn't exist, then we'll run analyzeData.coOccurrenceMatrix.\n",
    "if not os.path.exists(coOccurrenceMatrixSavePath):\n",
    "    # generate the coOccurrenceMatrix, using regexSearchResultsDataMatrix from the previously performed / loaded regex search\n",
    "    # as before, the assumption is that terms are the rows, and the award numbers are the columns\n",
    "    # NOTE: THIS MEANS `rowsOrColumns` SHOULD BE SET TO 'rows'\n",
    "    analyzeData.coOccurrenceMatrix(thresholdedData,rowsOrColumns='rows',savePath=coOccurrenceMatrixSavePath)\n",
    "# in either case, load up the coOccurrenceMatrix\n",
    "coOccurrenceMatrix=pd.read_csv(coOccurrenceMatrixSavePath,index_col=0,header='infer')\n",
    "# convert the content of the pandas dataframe to float\n",
    "coOccurrenceMatrix=coOccurrenceMatrix.astype(float)\n",
    "# because we know this is the term co-occurrence matrix, we can go ahead and set the diagonal to nan\n",
    "import numpy as np\n",
    "np.fill_diagonal(coOccurrenceMatrix.values,np.nan)\n",
    "# we'll just hold on to that for now, and move on to the more complicated case: term-agency co-occurrence\n",
    "\n",
    "# check if the agencyExtractSavePath file exists. If it doesn't exist, then we'll run analyzeData.fieldExtractAndSave.\n",
    "awardSubOrgFieldSequence=[]\n",
    "awardSubOrgFieldSequence.extend([rootTags[defaultDataIndex]])\n",
    "[awardSubOrgFieldSequence.extend(x) for x in subOrgFields[defaultDataIndex]]\n",
    "currSubOrgField=awardSubOrgFieldSequence\n",
    "# generate the nameFieldSequence\n",
    "nameFieldSequence=[]\n",
    "nameFieldSequence.extend([rootTags[defaultDataIndex]])\n",
    "[nameFieldSequence.extend(x) for x in nameFields[defaultDataIndex]]\n",
    "currNameFieldFull=nameFieldSequence\n",
    "if not os.path.exists(agencyExtractSavePath):\n",
    "    # generate the agencyExtract data, using fieldExtractAndSave(inputStructs,targetField,nameField,savePath)\n",
    "    analyzeData.fieldExtractAndSave(processedDataDirContainsXML,currSubOrgField,currNameFieldFull,agencyExtractSavePath)\n",
    "# in either case, load up the agencyExtract.  Given that it is a single column, it's always saved as a csv\n",
    "agencyExtract=pd.read_csv(agencyExtractSavePath,header='infer',index_col=False,dtype=str)\n",
    "# it should have two columns, the first is the award number ('itemID'), and the second is the agency ('fieldValue')\n",
    "\n",
    "# now that we have that, check to see if the agencyTermCoOccurrenceMatrixSavePath file exists. If it doesn't exist, then we'll run analyzeData.coOccurrenceMatrix.\n",
    "if not os.path.exists(agencyTermCoOccurrenceMatrixSavePath):\n",
    "    # for sumMergeMatrix_byCategories, we'll need to create a `matrix` variable, which is equivalent to our `regexSearchResultsDataMatrix` *except that*\n",
    "    # it's a pandas dataframe, with the rows and columns taken from the `rowName` and `colName` attributes of the `regexSearchResults` hdf5 file\n",
    "    # [x.decode('utf-8') for x in grantIDs]\n",
    "\n",
    "    analyzeData.sumMergeMatrix_byCategories(thresholdedData,agencyExtract,targetAxis='columns',savePath=agencyTermCoOccurrenceMatrixSavePath)\n",
    "\n",
    "# in either case, load up the agencyTermCoOccurrenceMatrix in the fashion appropriate for the file extension\n",
    "if agencyTermCoOccurrenceMatrixSavePath.endswith('.csv'):\n",
    "    agencyTermCoOccurrenceMatrix=pd.read_csv(agencyTermCoOccurrenceMatrixSavePath,index_col=0,header='infer')\n",
    "elif agencyTermCoOccurrenceMatrixSavePath.endswith('.hd5'):\n",
    "    agencyTermCoOccurrenceMatrixLoad=pd.read_hdf(agencyTermCoOccurrenceMatrixSavePath)\n",
    "    # let's quickly reshape the hd5 output to a pandas dataframe; the row and column names will come from the 'rowName' and 'colName' datasets, whereas the main dataset will come out as 'dataMatrix'\n",
    "    agencyTermCoOccurrenceMatrix=pd.DataFrame(data=agencyTermCoOccurrenceMatrixLoad['dataMatrix'][:],index=[x.decode('utf-8') for x in agencyTermCoOccurrenceMatrixLoad['rowName'][:]],columns=[x.decode('utf-8') for x in agencyTermCoOccurrenceMatrixLoad['colName'][:]])\n",
    "    # keep in mind the row and column names are in there as independent datasets, so you'll need to get those out too if needed. \n",
    "\n",
    "# create savepaths for both figures\n",
    "termTermCoOccurrenceMatrixFigSavePath=os.path.join(figuresDataPath,'termTermCoOccurrenceMatrixFig.svg')\n",
    "agencyTermCoOccurrenceMatrixFigSavePath=os.path.join(figuresDataPath,'agencyTermCoOccurrenceMatrixFig.svg')\n",
    "# now we use plot functions to take care of the rest, theoretically we could try and do these as subplots, but that likely won't work well considering the colorbars\n",
    "# first we'll do the term-term co-occurrence\n",
    "# plotCoOccurrenceMatrix(coOccurrenceMatrix,figSize=(10,10),rowTitle='Keywords',colTitle='Keywords',fig=None,ax=None,figSavePath=termTermCoOccurrenceMatrixFigSavePath)\n",
    "termTermCoOccurrenceMatrixFig=figs.coOccurrenceMatrix_heatmapPlot(coOccurrenceMatrix,figSize=None,logScale=None,rowTitle='Keywords',colTitle='Keywords',fig=None,ax=None,figSavePath=termTermCoOccurrenceMatrixFigSavePath)\n",
    "# now we'll do the agency-term co-occurrence\n",
    "# plotCoOccurrenceMatrix(agencyTermCoOccurrenceMatrix,figSize=(10,10),rowTitle='Keywords',colTitle='Agency',fig=None,ax=None,figSavePath=agencyTermCoOccurrenceMatrixFigSavePath)\n",
    "agencyTermCoOccurrenceMatrixFig=figs.coOccurrenceMatrix_heatmapPlot(agencyTermCoOccurrenceMatrix,figSize=None,logScale=None,rowTitle='Keywords',colTitle='Agency',fig=None,ax=None,figSavePath=agencyTermCoOccurrenceMatrixFigSavePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f619f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "agencyExtract.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20ad92cf",
   "metadata": {},
   "source": [
    "### How do our sub-organizations compare\n",
    "One question we might ask is if sub-organizations are mentioning these terms in different ways--if the patterns of co-occurrence are _different_ for different agencies.  To answer this question we can take the co-occurrence matrix _for each agency_ and compare them to one another (thus resulting in _another_ matrix, this time agency by agency).  The proper tool for this is called the [cosine distance or cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). This measure provides a measure of distance between two collections of equally sized / shaped quantificaitons (in this case the [unrolled](https://numpy.org/doc/stable/reference/generated/numpy.ravel.html) co-occurrence matrix.  Also, because we don't want the total number of grants for a particular agency to impact this analysis, we'll normalize the vectors (this is likely unnecessary due to how cosine distance works).\n",
    "\n",
    "#### Interpreting the plot\n",
    "\n",
    "Overall, what the coloration of the plot will indicate is the degree of similarity or difference in the patterns of term co-occurance for open-science related terms.  Implicitly, we might assume that a high degree of similarity would reflect agencies talking about open science topics in the same way, or focusing on the same aspects.  A high degree of difference would indicate using the terms in differing ways, potentially reflecting differing foci, or even different senses of the words being used (e.g. not in a sense related to open science).  Given that we are plotting _distance_ a value of 0 indicates overlap, and thus maximal similarity.  For this same reason, a value of 1 would the most extreme distance, and thus reflect maximal difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273f1e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to plot the results using seaborn\n",
    "# start by loading the hdf5 file with the results\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "# recall this is how we set the path previously\n",
    "hd5Path=os.path.join(localDataDir,'grantsGov','processed',currenthSearchList + '_results.h5')\n",
    "# load the file\n",
    "saveDictionary=h5py.File(hd5Path,'r')\n",
    "\n",
    "# also load the agencies\n",
    "#hd5Path=os.path.join(localDataDir,'grantsGov','processed','grantsGov_agencies.h5')\\\n",
    "# load the file\n",
    "#agencyDictionary=h5py.File(hd5Path,'r')\n",
    "# NOTE actually we changed this, because a 2D matrix is super inefficient when each row / grant can only have a single agency\n",
    "# as such we are now using a csv file\n",
    "agencyPath=os.path.join(localDataDir,'grantsGov','processed','grantsGov_agencies.csv')\n",
    "agencyDictionary=pd.read_csv(agencyPath)\n",
    "\n",
    "\n",
    "# we'll produce a bar plot where the x axis is the number of term matches in a given grant, and the y axis is the number of grants with that number of term matches\n",
    "# additionally, we'll color subsections of the bars by the agency\n",
    "# we'll start by getting the list of agencies\n",
    "# this is a list of strings\n",
    "agencyList=np.unique(agencyDictionary['fieldValue'].tolist())\n",
    "# next create a mapping for the colors\n",
    "# we'll use the seaborn color palette\n",
    "# first get the number of agencies\n",
    "numAgencies=len(agencyList)\n",
    "# then get the color palette\n",
    "colorPalette=sns.color_palette(\"hls\", numAgencies)\n",
    "\n",
    "# next we need to ensure that the data structure can be plotted as a stacked bar plot\n",
    "# we'll start by counting the number of term matches for each grant\n",
    "# this is a vector of integers\n",
    "termMatchCounts=np.count_nonzero(saveDictionary['dataMatrix'],axis=0)\n",
    "# lets create a pandas dataframe to hold the data we will plot\n",
    "# the rows will correspond to the number of term matches, while the columns will correspond to the agencies\n",
    "# we'll start by getting the maximum number of term matches\n",
    "maxTermMatches=np.max(termMatchCounts)\n",
    "# next we'll create a pandas dataframe with, where the column names come from agencyList\n",
    "# and the row names come from the number of term matches\n",
    "# we'll initialize the dataframe with zeros\n",
    "plotData=pd.DataFrame(np.zeros((maxTermMatches,numAgencies)),columns=agencyList)\n",
    "# next we also need to make sure that the entries in agencyDictionary['itemID']\n",
    "# are in the same order as the the columns / grants in saveDictionary['dataMatrix']\n",
    "# the specific grantIDs for saveDictionary are found in saveDictionary['colName']\n",
    "# we'll start by getting the grantIDs from saveDictionary\n",
    "# hopefully these are strings and not bytes\n",
    "grantIDs=saveDictionary['colName'][:]\n",
    "# if the first entry is a byte, we'll convert all of them to strings\n",
    "if type(grantIDs[0])==bytes:\n",
    "    grantIDs=[x.decode('utf-8') for x in grantIDs]\n",
    "    \n",
    "# next we'll get the grantIDs from agencyDictionary\n",
    "agencyGrantIDs=agencyDictionary['itemID'].tolist()\n",
    "# we'll do a quick check to make sure that the two lists are the same\n",
    "if not grantIDs==agencyGrantIDs:\n",
    "    # if they are not the same, we'll need to reorder the rows of agencyDictionary\n",
    "    # we'll start by getting the indices that would sort grantIDs\n",
    "    sortIndices=np.argsort(grantIDs)\n",
    "    # then we'll get the indices that would sort agencyGrantIDs\n",
    "    agencySortIndices=np.argsort(agencyGrantIDs)\n",
    "    # then we'll chain these to get the indices that would sort agencyDictionary to the same order as saveDictionary\n",
    "    correctSortIndices=np.argsort(agencySortIndices[sortIndices])\n",
    "    # then we'll use these indices to reorder agencyDictionary\n",
    "    agencyDictionary=agencyDictionary[correctSortIndices,:]\n",
    "# NOTE: Maybe none of this is necessary\n",
    "\n",
    "\n",
    "# next we'll iterate over the agencies \n",
    "for i in range(numAgencies):\n",
    "    # set the current agency\n",
    "    currentAgency=agencyList[i]\n",
    "    # get the indices of in the values in agencyDictionary['fieldValue'] that match currentAgency\n",
    "    currentIndices=np.where(agencyDictionary['fieldValue'][0,:]==currentAgency)[0]\n",
    "    # use these indicies on agencyDictionary['itemID'] to get the grantIDs\n",
    "    currentGrantIDs=agencyDictionary['itemID'][0,currentIndices]\n",
    "    # then use these grantIDs to get the indices of the grants in saveDictionary['colName']\n",
    "    currentAgencyGrantIndices=[grantIDs.index(x) for x in currentGrantIDs]\n",
    "    # then use these indices to get the number of term matches for the grants\n",
    "    currentTermMatchCounts=termMatchCounts[currentAgencyGrantIndices]\n",
    "    # then iterate over the range between 0 and maxTermMatches and fill in the associated rows of plotData with the counts\n",
    "    for j in range(0,maxTermMatches+1):\n",
    "        # check to see if j is even in currentTermMatchCounts\n",
    "        if j in currentTermMatchCounts:\n",
    "            # if it is, then get the number of times it occurs\n",
    "            currentCount=np.count_nonzero(currentTermMatchCounts==j)\n",
    "            # then set the corresponding entry in plotData to this count\n",
    "            plotData[currentAgency][j]=currentCount\n",
    "\n",
    "# now we can plot the data\n",
    "\n",
    "# set the seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "# set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "# set the x axis label\n",
    "plt.xlabel('Number of term matches')\n",
    "# set the y axis label\n",
    "plt.ylabel('Number of grants')\n",
    "# set the title\n",
    "plt.title('Number of term matches per grant')\n",
    "\n",
    "# we'll use an iterative approach to plot the data\n",
    "# this will require a bit of trickery, but we shold be fine\n",
    "# we'll iterate over the agencies\n",
    "# but first we need to create a vector to store the total count of grants for each count number, it will start as a vector of zeros\n",
    "totalCount=np.zeros(maxTermMatches+1)\n",
    "# then we'll iterate over the agencies\n",
    "for i in range(numAgencies):\n",
    "    # set the current agency\n",
    "    currentAgency=agencyList[i]\n",
    "    # set the current color\n",
    "    currentColor=colorPalette[i]\n",
    "    # create a temporary dataframe with the current agency, where the are two columns, \n",
    "    # the first simply has the number of term matches, and the second has the number of grants with that number of term matches\n",
    "    tempData=pd.DataFrame({'termMatches':plotData[currentAgency].index,'grantCount':plotData[currentAgency].values})\n",
    "    # BEFORE WE PLOT THOUGH\n",
    "    # we need to recompute and reset the grant counts, to account for the fact that we are plotting a stacked bar plot\n",
    "    # we'll start by getting the grant counts\n",
    "    currentGrantCounts=tempData['grantCount'].values\n",
    "    # then we'll add them to the totalCount\n",
    "    totalCount=totalCount+currentGrantCounts\n",
    "    # then we'll set the grant counts to the totalCount\n",
    "    tempData['grantCount']=totalCount\n",
    "    # then we'll plot the data\n",
    "    plt.bar(tempData['termMatches'],tempData['grantCount'],color=currentColor,label=currentAgency)\n",
    "\n",
    "# then we'll add the legend\n",
    "plt.legend(loc='upper right')\n",
    "# then we'll save the figure\n",
    "plt.savefig(os.path.join(localDataDir,'grantsGov','figures','termMatchCounts.png'))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveDictionary['colName'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158eacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(countsList)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a47073d5",
   "metadata": {},
   "source": [
    "#### With a simple keyword search\n",
    "\n",
    "We can make a first, naieve attempt at this by using a word-bank with words or phrases that we believe are likely to show up in a grant relevant to open science infrastructure (e.g. 'metadata', 'FAIR', 'data sharing', etc.).  In this repository we have just such a word-bank already available (in `keywords.txt`), but users should feel free to update and/or augment it as they see fit.\n",
    "\n",
    "To begin then, we'll do a quick search to see how many grants each of the word-bank terms comes up in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# open the keywords file\n",
    "with open('../keywords.txt', 'r') as f:\n",
    "    keywords = f.read()\n",
    "\n",
    "# split it into a list.  Each term is kept on a separate line\n",
    "keywords=keywords.split('\\n')\n",
    "\n",
    "# create a dictionary which will be saved as a json, so that you don't have to do this each time\n",
    "grantFindsOut={}\n",
    "\n",
    "# iterate through the keywords\n",
    "for iKeywords in keywords:\n",
    "    # create a blank list to store the IDs of the grants with the keyword in the description\n",
    "    grantsFound=[]\n",
    "    compiledSearch=re.compile('\\\\b'+iKeywords.lower()+'\\\\b')\n",
    "    for iRow,iListing in grantsDF.iterrows():\n",
    "        # maybe it doesn't have a description field\n",
    "        try:\n",
    "            # case insensitive find for the keyword\n",
    "            if bool(compiledSearch.search(iListing['Description'].lower().replace('-',''))):\n",
    "                #append the ID if found\n",
    "                grantsFound.append(iListing['OpportunityID'])\n",
    "        except:\n",
    "            # do nothing, if there's no description field, then the word can't be found\n",
    "            pass\n",
    "            \n",
    "    # store the found entries in the output dictionary.  Use the keyword as the key (with spaces replaced with underscores),\n",
    "    # and the value being the list of grant IDs\n",
    "    grantFindsOut[iKeywords.replace(' ','_')]=grantsFound\n",
    "\n",
    "# save it out\n",
    "with open(\"grantFindsOut.json\", \"w\") as outfile:\n",
    "    json.dump(grantFindsOut, outfile)\n",
    "    \n",
    "# plot a histogram\n",
    "#silly way to do this, but seaborn was giving me issues\n",
    "import itertools\n",
    "keywordCountVec=[]\n",
    "for iKeywords in list(grantFindsOut.keys()):\n",
    "    currVec=[iKeywords.replace('_','\\n')] * len(grantFindsOut[iKeywords])\n",
    "    keywordCountVec.extend(currVec)\n",
    "\n",
    "import seaborn as sns\n",
    "keywordHistDF=pd.DataFrame(data=keywordCountVec, columns=['keyword'])\n",
    "#keywordHistDF.loc[0:10]=[len(grantFindsOut[iKeyword]) for iKeyword in grantFindsOut.keys()]\n",
    "#keywordHistDF\n",
    "sns.set(rc={'figure.figsize':(25,10)})\n",
    "ax=sns.countplot(keywordCountVec)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=50, ha=\"right\",rotation_mode='anchor',fontsize=18, linespacing=.8)\n",
    "#ax.set_yticklabels(ax.get_yticklabels(),fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', pad=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b99ac985",
   "metadata": {},
   "source": [
    "#### The resulting plot\n",
    "\n",
    "In the resulting plot we get a rough sense of how frequently these terms are coming up.  Admittedly, we don't actually know the context in which these terms are being used, so it's quite possible that they are being used with a different meaning in mind (e.g. \"fair\").  That being said, it's a only starting point, and maybe we can do better.\n",
    "\n",
    "### Co-occurance\n",
    "\n",
    "Perhaps it's not sufficient to know if _any_ of the relevant terms shows up in the gant description.  Maybe instead, we might want to require some number or subset of these terms to co-occur to help ensure that we're actually targeting relevant grants.  To consider this, we would want to look at which of these terms occur together and how frequently they do so.\n",
    "\n",
    "Lets start with a basic matrix plot of these relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee51a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from matplotlib.colors import LogNorm\n",
    "# ok, lets create a matrix that holds co-occurance data\n",
    "# let's start by making a blank matrix to hold the data\n",
    "connectivityMatrix=np.zeros((len(keywords),len(keywords)), dtype=np.int32())\n",
    "# also make a list to hold the rows from this\n",
    "dfRowContentAll=[]\n",
    "# should be symmetric so x and y doesn't really matter\n",
    "# use this opportunity to get the indexes of the lower triangle though\n",
    "lowerTriIndices = [list(x) for x in np.tril_indices_from(connectivityMatrix)]\n",
    "#convert this to the format we need\n",
    "lowerTriList=[[lowerTriIndices[0][iIndex],lowerTriIndices[1][iIndex]] for iIndex in range(len(lowerTriIndices[0]))]\n",
    "# iterate across both axes\n",
    "\n",
    "for iIndexX, iKeywordsX in enumerate(keywords):\n",
    "    for iIndexY, iKeywordsY in enumerate(keywords):\n",
    "        # get the values for each \"node\" (e.g. the grant IDs for each keyword)\n",
    "        IDsX=grantFindsOut[iKeywordsX.replace(' ','_')]\n",
    "        IDsY=grantFindsOut[iKeywordsY.replace(' ','_')]\n",
    "        # find the intersection\n",
    "        intersectionGrants=list(set(IDsX) & set(IDsY))\n",
    "        # find the size of that intersection\n",
    "        sharedGrantNum=len(intersectionGrants)\n",
    "        connectivityMatrix[iIndexX,iIndexY]=sharedGrantNum\n",
    "        # do components for dataframe\n",
    "\n",
    "        if not [iIndexX,iIndexY] in lowerTriList:\n",
    "            print \n",
    "            \n",
    "            if iKeywordsX==iKeywordsY:\n",
    "                # half it for the visualization for self connections\n",
    "                #dfRowContent=[iKeywordsX, iKeywordsY, np.divide(sharedGrantNum,2).astype(np.int32)]\n",
    "                # or set it to zero\n",
    "                dfRowContent=[iKeywordsX + ' (' + str(len(IDsX)) + ')', iKeywordsY + ' (' + str(len(IDsY)) + ')' , 0]\n",
    "            else:\n",
    "                dfRowContent=[iKeywordsX + ' (' + str(len(IDsX)) + ')', iKeywordsY + ' (' + str(len(IDsY)) + ')', sharedGrantNum]\n",
    "            dfRowContentAll.append(dfRowContent)\n",
    "\n",
    "flatConMatrix=pd.DataFrame(data=dfRowContentAll, columns=['keyword1','keyword2','value'])\n",
    "\n",
    "# mask out the diagonal so it doesn't overwhelm the plot\n",
    "diagonalMask=np.eye(len(keywords),dtype=bool)\n",
    "# copy the matrix so it can be modified \n",
    "plotMatrix=copy.deepcopy(connectivityMatrix)\n",
    "# set the diagonal to zero\n",
    "plotMatrix[diagonalMask]=np.zeros(len(keywords))\n",
    "# replace zero with empty, to clean up plot                \n",
    "\n",
    "    \n",
    "sns.heatmap(data=plotMatrix,yticklabels=keywords,xticklabels=keywords,cmap='viridis',norm=LogNorm(),cbar_kws={'label': 'Grant Count\\n(log-scaled)'})\n",
    "plt.gcf().get_axes()[0].set_xticklabels(plt.gcf().get_axes()[0].get_xticklabels(), fontsize=22)\n",
    "plt.gcf().get_axes()[0].set_title('Open science term co-occurrences',fontsize=28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400a096",
   "metadata": {},
   "source": [
    "#### A different view\n",
    "\n",
    "The above matrix plot provides a good sense of the quantative characteristics of term co-occurance in the grants.  However, this sort of visualization may not completely encapsulate the overall patterns that we may be looking for.  For an alternate perspective we can try a chord diagram.\n",
    "\n",
    "NOTE:  The code block below makes use of [d3blocks](https://github.com/d3blocks/d3blocks), which is a python package that is unlikely to be installed by default.  Also note, that this code block will open up a new window with the resulting figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1665338",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from d3blocks import D3Blocks\n",
    "import os\n",
    "# Initialize\n",
    "d3 = D3Blocks()\n",
    "# change the column names to what's required by d3blocks\n",
    "# https://d3blocks.github.io/d3blocks/pages/html/Chord.html\n",
    "flatConMatrix=flatConMatrix.rename(columns={\"keyword1\": \"source\", \"keyword2\": \"target\", 'value': 'weight'})\n",
    "# notebook= True doesn't seem to work\n",
    "# d3.chord(flatConMatrix,filepath=None,notebook=True)\n",
    "d3.chord(flatConMatrix, showfig=False,filepath='./d3blocks.html')\n",
    "\n",
    "import IPython\n",
    "IPython.display.IFrame('d3blocks.html',height=900,width=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
